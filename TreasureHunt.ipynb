{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TreasureHunt.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMaSzbEkzGw1YNKlnGoCRDB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kailing231/treasurecube/blob/main/TreasureHunt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiPfwcfVgNvV"
      },
      "source": [
        "# environment.py\n",
        "\n",
        "import numpy as np\n",
        "import time\n",
        "import random\n",
        "\n",
        "from abc import ABC, abstractmethod\n",
        "\n",
        "\n",
        "class AbstractEnvironment(ABC):\n",
        "    def __init__(self):\n",
        "        self.agent_sign = '+'\n",
        "        self.goal_sign = 'G'\n",
        "        self.corridor_sign = '-'\n",
        "\n",
        "    def render(self):\n",
        "        raise NotImplemented\n",
        "\n",
        "    def reset(self):\n",
        "        raise NotImplemented\n",
        "\n",
        "    def step(self, action):\n",
        "        raise NotImplemented\n",
        "\n",
        "class TreasureCube(AbstractEnvironment):\n",
        "    def __init__(self, max_step=20):\n",
        "        super(TreasureCube, self).__init__()\n",
        "        self.dim = 4\n",
        "        self.max_step = max_step\n",
        "        self.curr_pos = [0, 0, 0]  # (z, x, y)\n",
        "        self.time_step = 0\n",
        "        self.end_pos = [self.dim - 1, self.dim - 1, self.dim - 1]\n",
        "        self.visual_state = []\n",
        "        self.seed = None\n",
        "        self.set_seed()\n",
        "        self.all_actions = ['right', 'left', 'up', 'down', 'forward', 'backward']\n",
        "        self.slip_actions = dict()\n",
        "        self.set_action_rules()\n",
        "\n",
        "    def reset(self):\n",
        "        self.curr_pos = [0, 0, 0]\n",
        "        self.time_step = 0\n",
        "        self.end_pos = [self.dim - 1, self.dim - 1, self.dim - 1]\n",
        "        self._reset_visual_states(self.curr_pos, self.end_pos)\n",
        "        return ''.join(str(pos) for pos in self.curr_pos)\n",
        "\n",
        "    # def randomState(self): # todo remove\n",
        "    #     x = random.randrange(0,3)\n",
        "    #     y = random.randrange(0,3)\n",
        "    #     z = random.randrange(0,3)\n",
        "    #     return [x,y,z]\n",
        "\n",
        "    def getNextState(self, state, action): # todo added\n",
        "        assert action in self.all_actions\n",
        "        curr_state = list(map(int,str(state)))      \n",
        "        if action == 'left':\n",
        "            if curr_state[1] == 0:  # wall\n",
        "                pass\n",
        "            else:\n",
        "                curr_state[1] -= 1\n",
        "        elif action == 'right':\n",
        "            if curr_state[1] == self.dim - 1:  # wall\n",
        "                pass\n",
        "            elif curr_state[1] == self.dim - 2 and curr_state[0] == self.dim - 1 and curr_state[\n",
        "                2] == self.dim - 1:\n",
        "                curr_state[1] += 1\n",
        "                # is_terminate = True\n",
        "                # reward = 1\n",
        "            else:\n",
        "                curr_state[1] += 1\n",
        "\n",
        "        elif action == 'forward':\n",
        "            if curr_state[0] == self.dim - 1:  # wall\n",
        "                pass\n",
        "            elif curr_state[0] == self.dim - 2 and curr_state[1] == self.dim - 1 and curr_state[\n",
        "                2] == self.dim - 1:\n",
        "                curr_state[0] += 1\n",
        "                # is_terminate = True\n",
        "                # reward = 1\n",
        "            else:\n",
        "                curr_state[0] += 1\n",
        "\n",
        "        elif action == 'backward':\n",
        "            if curr_state[0] == 0:  # wall\n",
        "                pass\n",
        "            else:\n",
        "                curr_state[0] -= 1\n",
        "\n",
        "        elif action == 'up':\n",
        "            if curr_state[2] == self.dim - 1:  # wall\n",
        "                pass\n",
        "            elif curr_state[2] == self.dim - 2 and curr_state[0] == self.dim - 1 and curr_state[\n",
        "                1] == self.dim - 1:\n",
        "                curr_state[2] += 1\n",
        "                # is_terminate = True\n",
        "                # reward = 1\n",
        "            else:\n",
        "                curr_state[2] += 1\n",
        "\n",
        "        elif action == 'down':\n",
        "            if curr_state[2] == 0:\n",
        "                pass\n",
        "            else:\n",
        "                curr_state[2] -= 1\n",
        "                \n",
        "        # self._reset_visual_states(self.curr_pos, self.end_pos)\n",
        "        next_state = ''.join(str(pos) for pos in curr_state)\n",
        "        return next_state\n",
        "    \n",
        "    def step(self, action, stochastic=True):\n",
        "        in_action = action  # action from agent\n",
        "        assert action in self.all_actions\n",
        "        reward = -0.1\n",
        "        is_terminate = False\n",
        "        pre_pos = self.curr_pos\n",
        "        r = random.random()\n",
        "        if action == 'right':\n",
        "            if r < 0.1:\n",
        "                action = 'up'\n",
        "            elif r < 0.2:\n",
        "                action = 'down'\n",
        "            elif r < 0.3:\n",
        "                action = 'forward'\n",
        "            elif r < 0.4:\n",
        "                action = 'backward'\n",
        "            else:\n",
        "                action = 'right'\n",
        "        elif action == 'left':\n",
        "            if r < 0.1:\n",
        "                action = 'up'\n",
        "            elif r < 0.2:\n",
        "                action = 'down'\n",
        "            elif r < 0.3:\n",
        "                action = 'forward'\n",
        "            elif r < 0.4:\n",
        "                action = 'backward'\n",
        "            else:\n",
        "                action = 'left'\n",
        "        elif action == 'up':\n",
        "            if r < 0.1:\n",
        "                action = 'left'\n",
        "            elif r < 0.2:\n",
        "                action = 'right'\n",
        "            elif r < 0.3:\n",
        "                action = 'forward'\n",
        "            elif r < 0.4:\n",
        "                action = 'backward'\n",
        "            else:\n",
        "                action = 'up'\n",
        "        elif action == 'down':\n",
        "            if r < 0.1:\n",
        "                action = 'left'\n",
        "            elif r < 0.2:\n",
        "                action = 'right'\n",
        "            elif r < 0.3:\n",
        "                action = 'forward'\n",
        "            elif r < 0.4:\n",
        "                action = 'backward'\n",
        "            else:\n",
        "                action = 'down'\n",
        "        elif action == 'forward':\n",
        "            if r < 0.1:\n",
        "                action = 'left'\n",
        "            elif r < 0.2:\n",
        "                action = 'right'\n",
        "            elif r < 0.3:\n",
        "                action = 'up'\n",
        "            elif r < 0.4:\n",
        "                action = 'down'\n",
        "            else:\n",
        "                action = 'forward'\n",
        "        else:\n",
        "            if r < 0.1:\n",
        "                action = 'left'\n",
        "            elif r < 0.2:\n",
        "                action = 'right'\n",
        "            elif r < 0.3:\n",
        "                action = 'up'\n",
        "            elif r < 0.4:\n",
        "                action = 'down'\n",
        "            else:\n",
        "                action = 'backward'\n",
        "\n",
        "        if not stochastic:\n",
        "            action = in_action\n",
        "\n",
        "        assert action in self.all_actions\n",
        "        if action == 'left':\n",
        "            if self.curr_pos[1] == 0:  # wall\n",
        "                pass\n",
        "            else:\n",
        "                self.curr_pos[1] -= 1\n",
        "        elif action == 'right':\n",
        "            if self.curr_pos[1] == self.dim - 1:  # wall\n",
        "                pass\n",
        "            elif self.curr_pos[1] == self.dim - 2 and self.curr_pos[0] == self.dim - 1 and self.curr_pos[\n",
        "                2] == self.dim - 1:\n",
        "                self.curr_pos[1] += 1\n",
        "                is_terminate = True\n",
        "                reward = 1\n",
        "            else:\n",
        "                self.curr_pos[1] += 1\n",
        "\n",
        "        elif action == 'forward':\n",
        "            if self.curr_pos[0] == self.dim - 1:  # wall\n",
        "                pass\n",
        "            elif self.curr_pos[0] == self.dim - 2 and self.curr_pos[1] == self.dim - 1 and self.curr_pos[\n",
        "                2] == self.dim - 1:\n",
        "                self.curr_pos[0] += 1\n",
        "                is_terminate = True\n",
        "                reward = 1\n",
        "            else:\n",
        "                self.curr_pos[0] += 1\n",
        "        elif action == 'backward':\n",
        "            if self.curr_pos[0] == 0:  # wall\n",
        "                pass\n",
        "            else:\n",
        "                self.curr_pos[0] -= 1\n",
        "\n",
        "        elif action == 'up':\n",
        "            if self.curr_pos[2] == self.dim - 1:  # wall\n",
        "                pass\n",
        "            elif self.curr_pos[2] == self.dim - 2 and self.curr_pos[0] == self.dim - 1 and self.curr_pos[\n",
        "                1] == self.dim - 1:\n",
        "                self.curr_pos[2] += 1\n",
        "                is_terminate = True\n",
        "                reward = 1\n",
        "            else:\n",
        "                self.curr_pos[2] += 1\n",
        "        elif action == 'down':\n",
        "            if self.curr_pos[2] == 0:\n",
        "                pass\n",
        "            else:\n",
        "                self.curr_pos[2] -= 1\n",
        "\n",
        "        assert action in self.all_actions\n",
        "        self.time_step += 1\n",
        "        if self.time_step == self.max_step - 1:\n",
        "            is_terminate = True\n",
        "\n",
        "        self._reset_visual_states(self.curr_pos, self.end_pos)\n",
        "        return reward, is_terminate, ''.join(str(pos) for pos in self.curr_pos)\n",
        "\n",
        "    def render(self):\n",
        "        print(' '.join(['*'] * self.dim))\n",
        "        for i in range(self.dim):\n",
        "            for line in self.visual_state[i]:\n",
        "                print(' '.join(line))\n",
        "            print(' '.join(['#'] * self.dim))\n",
        "        print(' '.join(['*'] * self.dim))\n",
        "\n",
        "    def set_seed(self, seed=10086):\n",
        "        self.seed = seed\n",
        "        random.seed(seed)\n",
        "\n",
        "    def _reset_visual_states(self, agent_pos, goal_pos):\n",
        "        self.visual_state = [[[self.corridor_sign] * self.dim for _ in range(self.dim)] for _ in range(self.dim)]\n",
        "        self.visual_state[agent_pos[0]][agent_pos[1]][agent_pos[2]] = self.agent_sign\n",
        "        self.visual_state[goal_pos[0]][goal_pos[1]][goal_pos[2]] = self.goal_sign\n",
        "\n",
        "    def set_action_rules(self):\n",
        "        self.slip_actions['right'] = ['up', 'down', 'forward', 'backward', 'right']\n",
        "        self.slip_actions['left'] = ['up', 'down', 'forward', 'backward', 'left']\n",
        "        self.slip_actions['up'] = ['left', 'right', 'forward', 'backward', 'up']\n",
        "        self.slip_actions['down'] = ['left', 'right', 'forward', 'backward', 'down']\n",
        "        self.slip_actions['forward'] = ['left', 'right', 'up', 'down', 'forward']\n",
        "        self.slip_actions['backward'] = ['left', 'right', 'up', 'down', 'backward']\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAUY_iV0h6r7"
      },
      "source": [
        "test.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtPeHBGVhAPt"
      },
      "source": [
        "import argparse\n",
        "import random\n",
        "\n",
        "import pandas as pd"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9FwlcIiiRJD"
      },
      "source": [
        "# # initial all 0's\n",
        "# # each coordinate value = [0,3]\n",
        "# # row = state, no of states = 4*4*4 = 64\n",
        "# # col = action, no of actions = 6\n",
        "\n",
        "# env = TreasureCube();\n",
        "# length = env.dim\n",
        "\n",
        "# indexnames = []\n",
        "# for x in range(length):\n",
        "#     for y in range(length):\n",
        "#         for z in range(length):\n",
        "#           indexnames.append(str(x) + str(y) + str(z))\n",
        "\n",
        "# Qtable = np.zeros([64, 6])\n",
        "# Qtable = pd.DataFrame(Qtable, index=indexnames, columns=env.all_actions)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18pJOzaNuDSD"
      },
      "source": [
        "# qdf = Qtable.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIdZJ9or0Nyu"
      },
      "source": [
        "# # Qtable.loc['000'] # find all action values, of the state\n",
        "# state = \"000\"\n",
        "# # Qtable.loc[state].idxmax()\n",
        "# action = \"left\"\n",
        "# # qdf.loc[state, action] = 999\n",
        "# qdf.loc[state].max()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8KO3i0708n7"
      },
      "source": [
        "# state = env.randomState()\n",
        "# \"\".join([str(s) for s in state]) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuqirUnIF2vM"
      },
      "source": [
        "class RandomAgent(object):\n",
        "    def __init__(self):\n",
        "        self.action_space = ['left','right','forward','backward','up','down'] # in TreasureCube\n",
        "        self.Q = []\n",
        "\n",
        "    def take_action(self, state):\n",
        "        action = random.choice(self.action_space)\n",
        "        return action\n",
        "\n",
        "    # implement your train/update function to update self.V or self.Q\n",
        "    # you should pass arguments to the train function\n",
        "#    def train(self, state, action, next_state, reward):\n",
        "    def train(self, state, action, next_state, reward, Qtable):\n",
        "#      pass\n",
        "      alpha = 0.5 # learning rate\n",
        "      gamma = 0.99 # discount factor\n",
        "\n",
        "      old_value = Qtable.loc[state, action]\n",
        "      next_max = Qtable.loc[next_state].max()\n",
        "      new_value = (1.0 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
        "      Qtable.loc[state, action] = new_value\n",
        "#      return Qtable # return updated Qtable ???? # todo try\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFdEbv5dsCHr",
        "outputId": "e0026352-0d15-4152-ac89-2145e0a38269",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# TODO\n",
        "max_episode = 500\n",
        "max_step = 500\n",
        "\n",
        "env = TreasureCube()\n",
        "agent = RandomAgent()\n",
        "\n",
        "# ADDED\n",
        "length = env.dim\n",
        "indexnames = []\n",
        "for x in range(length):\n",
        "    for y in range(length):\n",
        "        for z in range(length):\n",
        "          indexnames.append(str(x) + str(y) + str(z))\n",
        "Qtable = np.zeros([64, 6])\n",
        "Qtable = pd.DataFrame(Qtable, index=indexnames, columns=env.all_actions)\n",
        "# END\n",
        "\n",
        "for epsisode_num in range(0, max_episode):\n",
        "  state = env.reset()\n",
        "  terminate = False\n",
        "  t = 0   # step number ? TODO remove\n",
        "  episode_reward = 0\n",
        "\n",
        "#        alpha = 0.5 # learning rate\n",
        "#        gamma = 0.99 # discount factor\n",
        "  epsilon = 0.01 # exploration rate # todo remove\n",
        "\n",
        "  while not terminate:\n",
        "      if random.uniform(0, 1) < epsilon:\n",
        "        action = agent.take_action(state) # choose a random action\n",
        "      else:\n",
        "#        action = np.argmax(q_table[state]) # Exploit learned values\n",
        "        action = Qtable.loc[state].idxmax()\n",
        "#      action = agent.take_action(state) # choose a random action // todo remove\n",
        "\n",
        "      reward, terminate, next_state = env.step(action)\n",
        "      episode_reward += reward            \n",
        "\n",
        "      # you can comment the following two lines, if the output is too much\n",
        "      # env.render() # comment\n",
        "      # print(f'step: {t}, action: {action}, reward: {reward}') # comment  \n",
        "\n",
        "      t += 1\n",
        "#            agent.train(state, action, next_state, reward)\n",
        "      agent.train(state, action, next_state, reward, Qtable)\n",
        "#      Qtable = agent.train(state, action, next_state, reward, Qtable)\n",
        "      state = next_state\n",
        "  print(f'epsisode: {epsisode_num}, total_steps: {t} episode reward: {episode_reward}')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epsisode: 0, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 1, total_steps: 15 episode reward: -0.40000000000000013\n",
            "epsisode: 2, total_steps: 11 episode reward: 1.1102230246251565e-16\n",
            "epsisode: 3, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 4, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 5, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 6, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 7, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 8, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 9, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 10, total_steps: 15 episode reward: -0.40000000000000013\n",
            "epsisode: 11, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 12, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 13, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 14, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 15, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 16, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 17, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 18, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 19, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 20, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 21, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 22, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 23, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 24, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 25, total_steps: 18 episode reward: -0.7000000000000004\n",
            "epsisode: 26, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 27, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 28, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 29, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 30, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 31, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 32, total_steps: 18 episode reward: -0.7000000000000004\n",
            "epsisode: 33, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 34, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 35, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 36, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 37, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 38, total_steps: 14 episode reward: -0.30000000000000004\n",
            "epsisode: 39, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 40, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 41, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 42, total_steps: 12 episode reward: -0.09999999999999987\n",
            "epsisode: 43, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 44, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 45, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 46, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 47, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 48, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 49, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 50, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 51, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 52, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 53, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 54, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 55, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 56, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 57, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 58, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 59, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 60, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 61, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 62, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 63, total_steps: 16 episode reward: -0.5000000000000002\n",
            "epsisode: 64, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 65, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 66, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 67, total_steps: 15 episode reward: -0.40000000000000013\n",
            "epsisode: 68, total_steps: 19 episode reward: -0.8000000000000005\n",
            "epsisode: 69, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 70, total_steps: 14 episode reward: -0.30000000000000004\n",
            "epsisode: 71, total_steps: 16 episode reward: -0.5000000000000002\n",
            "epsisode: 72, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 73, total_steps: 15 episode reward: -0.40000000000000013\n",
            "epsisode: 74, total_steps: 15 episode reward: -0.40000000000000013\n",
            "epsisode: 75, total_steps: 16 episode reward: -0.5000000000000002\n",
            "epsisode: 76, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 77, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 78, total_steps: 19 episode reward: -0.8000000000000005\n",
            "epsisode: 79, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 80, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 81, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 82, total_steps: 16 episode reward: -0.5000000000000002\n",
            "epsisode: 83, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 84, total_steps: 15 episode reward: -0.40000000000000013\n",
            "epsisode: 85, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 86, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 87, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 88, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 89, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 90, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 91, total_steps: 13 episode reward: -0.19999999999999996\n",
            "epsisode: 92, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 93, total_steps: 12 episode reward: -0.09999999999999987\n",
            "epsisode: 94, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 95, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 96, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 97, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 98, total_steps: 17 episode reward: -0.6000000000000003\n",
            "epsisode: 99, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 100, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 101, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 102, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 103, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 104, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 105, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 106, total_steps: 12 episode reward: -0.09999999999999987\n",
            "epsisode: 107, total_steps: 13 episode reward: -0.19999999999999996\n",
            "epsisode: 108, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 109, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 110, total_steps: 17 episode reward: -0.6000000000000003\n",
            "epsisode: 111, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 112, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 113, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 114, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 115, total_steps: 12 episode reward: -0.09999999999999987\n",
            "epsisode: 116, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 117, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 118, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 119, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 120, total_steps: 13 episode reward: -0.19999999999999996\n",
            "epsisode: 121, total_steps: 15 episode reward: -0.40000000000000013\n",
            "epsisode: 122, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 123, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 124, total_steps: 14 episode reward: -0.30000000000000004\n",
            "epsisode: 125, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 126, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 127, total_steps: 17 episode reward: -0.6000000000000003\n",
            "epsisode: 128, total_steps: 17 episode reward: -0.6000000000000003\n",
            "epsisode: 129, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 130, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 131, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 132, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 133, total_steps: 11 episode reward: 1.1102230246251565e-16\n",
            "epsisode: 134, total_steps: 16 episode reward: -0.5000000000000002\n",
            "epsisode: 135, total_steps: 15 episode reward: -0.40000000000000013\n",
            "epsisode: 136, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 137, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 138, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 139, total_steps: 17 episode reward: -0.6000000000000003\n",
            "epsisode: 140, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 141, total_steps: 12 episode reward: -0.09999999999999987\n",
            "epsisode: 142, total_steps: 19 episode reward: -0.8000000000000005\n",
            "epsisode: 143, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 144, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 145, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 146, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 147, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 148, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 149, total_steps: 18 episode reward: -0.7000000000000004\n",
            "epsisode: 150, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 151, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 152, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 153, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 154, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 155, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 156, total_steps: 17 episode reward: -0.6000000000000003\n",
            "epsisode: 157, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 158, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 159, total_steps: 18 episode reward: -0.7000000000000004\n",
            "epsisode: 160, total_steps: 13 episode reward: -0.19999999999999996\n",
            "epsisode: 161, total_steps: 18 episode reward: -0.7000000000000004\n",
            "epsisode: 162, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 163, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 164, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 165, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 166, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 167, total_steps: 9 episode reward: 0.20000000000000007\n",
            "epsisode: 168, total_steps: 12 episode reward: -0.09999999999999987\n",
            "epsisode: 169, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 170, total_steps: 16 episode reward: -0.5000000000000002\n",
            "epsisode: 171, total_steps: 13 episode reward: -0.19999999999999996\n",
            "epsisode: 172, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 173, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 174, total_steps: 19 episode reward: -0.8000000000000005\n",
            "epsisode: 175, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 176, total_steps: 10 episode reward: 0.10000000000000009\n",
            "epsisode: 177, total_steps: 12 episode reward: -0.09999999999999987\n",
            "epsisode: 178, total_steps: 14 episode reward: -0.30000000000000004\n",
            "epsisode: 179, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 180, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 181, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 182, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 183, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 184, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 185, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 186, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 187, total_steps: 15 episode reward: -0.40000000000000013\n",
            "epsisode: 188, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 189, total_steps: 16 episode reward: -0.5000000000000002\n",
            "epsisode: 190, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 191, total_steps: 18 episode reward: -0.7000000000000004\n",
            "epsisode: 192, total_steps: 16 episode reward: -0.5000000000000002\n",
            "epsisode: 193, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 194, total_steps: 13 episode reward: -0.19999999999999996\n",
            "epsisode: 195, total_steps: 14 episode reward: -0.30000000000000004\n",
            "epsisode: 196, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 197, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 198, total_steps: 12 episode reward: -0.09999999999999987\n",
            "epsisode: 199, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 200, total_steps: 18 episode reward: -0.7000000000000004\n",
            "epsisode: 201, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 202, total_steps: 13 episode reward: -0.19999999999999996\n",
            "epsisode: 203, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 204, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 205, total_steps: 13 episode reward: -0.19999999999999996\n",
            "epsisode: 206, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 207, total_steps: 17 episode reward: -0.6000000000000003\n",
            "epsisode: 208, total_steps: 15 episode reward: -0.40000000000000013\n",
            "epsisode: 209, total_steps: 9 episode reward: 0.20000000000000007\n",
            "epsisode: 210, total_steps: 11 episode reward: 1.1102230246251565e-16\n",
            "epsisode: 211, total_steps: 12 episode reward: -0.09999999999999987\n",
            "epsisode: 212, total_steps: 14 episode reward: -0.30000000000000004\n",
            "epsisode: 213, total_steps: 13 episode reward: -0.19999999999999996\n",
            "epsisode: 214, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 215, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 216, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 217, total_steps: 12 episode reward: -0.09999999999999987\n",
            "epsisode: 218, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 219, total_steps: 18 episode reward: -0.7000000000000004\n",
            "epsisode: 220, total_steps: 10 episode reward: 0.10000000000000009\n",
            "epsisode: 221, total_steps: 15 episode reward: -0.40000000000000013\n",
            "epsisode: 222, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 223, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 224, total_steps: 19 episode reward: -0.8000000000000005\n",
            "epsisode: 225, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 226, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 227, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 228, total_steps: 16 episode reward: -0.5000000000000002\n",
            "epsisode: 229, total_steps: 19 episode reward: -0.8000000000000005\n",
            "epsisode: 230, total_steps: 16 episode reward: -0.5000000000000002\n",
            "epsisode: 231, total_steps: 14 episode reward: -0.30000000000000004\n",
            "epsisode: 232, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 233, total_steps: 16 episode reward: -0.5000000000000002\n",
            "epsisode: 234, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 235, total_steps: 9 episode reward: 0.20000000000000007\n",
            "epsisode: 236, total_steps: 13 episode reward: -0.19999999999999996\n",
            "epsisode: 237, total_steps: 19 episode reward: -0.8000000000000005\n",
            "epsisode: 238, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 239, total_steps: 12 episode reward: -0.09999999999999987\n",
            "epsisode: 240, total_steps: 13 episode reward: -0.19999999999999996\n",
            "epsisode: 241, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 242, total_steps: 18 episode reward: -0.7000000000000004\n",
            "epsisode: 243, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 244, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 245, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 246, total_steps: 19 episode reward: -0.8000000000000005\n",
            "epsisode: 247, total_steps: 14 episode reward: -0.30000000000000004\n",
            "epsisode: 248, total_steps: 19 episode reward: -0.8000000000000005\n",
            "epsisode: 249, total_steps: 15 episode reward: -0.40000000000000013\n",
            "epsisode: 250, total_steps: 13 episode reward: -0.19999999999999996\n",
            "epsisode: 251, total_steps: 17 episode reward: -0.6000000000000003\n",
            "epsisode: 252, total_steps: 14 episode reward: -0.30000000000000004\n",
            "epsisode: 253, total_steps: 16 episode reward: -0.5000000000000002\n",
            "epsisode: 254, total_steps: 12 episode reward: -0.09999999999999987\n",
            "epsisode: 255, total_steps: 15 episode reward: -0.40000000000000013\n",
            "epsisode: 256, total_steps: 17 episode reward: -0.6000000000000003\n",
            "epsisode: 257, total_steps: 13 episode reward: -0.19999999999999996\n",
            "epsisode: 258, total_steps: 13 episode reward: -0.19999999999999996\n",
            "epsisode: 259, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 260, total_steps: 10 episode reward: 0.10000000000000009\n",
            "epsisode: 261, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 262, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 263, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 264, total_steps: 17 episode reward: -0.6000000000000003\n",
            "epsisode: 265, total_steps: 14 episode reward: -0.30000000000000004\n",
            "epsisode: 266, total_steps: 10 episode reward: 0.10000000000000009\n",
            "epsisode: 267, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 268, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 269, total_steps: 17 episode reward: -0.6000000000000003\n",
            "epsisode: 270, total_steps: 11 episode reward: 1.1102230246251565e-16\n",
            "epsisode: 271, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 272, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 273, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 274, total_steps: 9 episode reward: 0.20000000000000007\n",
            "epsisode: 275, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 276, total_steps: 16 episode reward: -0.5000000000000002\n",
            "epsisode: 277, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 278, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 279, total_steps: 15 episode reward: -0.40000000000000013\n",
            "epsisode: 280, total_steps: 10 episode reward: 0.10000000000000009\n",
            "epsisode: 281, total_steps: 17 episode reward: -0.6000000000000003\n",
            "epsisode: 282, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 283, total_steps: 15 episode reward: -0.40000000000000013\n",
            "epsisode: 284, total_steps: 12 episode reward: -0.09999999999999987\n",
            "epsisode: 285, total_steps: 11 episode reward: 1.1102230246251565e-16\n",
            "epsisode: 286, total_steps: 12 episode reward: -0.09999999999999987\n",
            "epsisode: 287, total_steps: 15 episode reward: -0.40000000000000013\n",
            "epsisode: 288, total_steps: 16 episode reward: -0.5000000000000002\n",
            "epsisode: 289, total_steps: 11 episode reward: 1.1102230246251565e-16\n",
            "epsisode: 290, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 291, total_steps: 18 episode reward: -0.7000000000000004\n",
            "epsisode: 292, total_steps: 15 episode reward: -0.40000000000000013\n",
            "epsisode: 293, total_steps: 17 episode reward: -0.6000000000000003\n",
            "epsisode: 294, total_steps: 11 episode reward: 1.1102230246251565e-16\n",
            "epsisode: 295, total_steps: 14 episode reward: -0.30000000000000004\n",
            "epsisode: 296, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 297, total_steps: 14 episode reward: -0.30000000000000004\n",
            "epsisode: 298, total_steps: 11 episode reward: 1.1102230246251565e-16\n",
            "epsisode: 299, total_steps: 17 episode reward: -0.6000000000000003\n",
            "epsisode: 300, total_steps: 13 episode reward: -0.19999999999999996\n",
            "epsisode: 301, total_steps: 14 episode reward: -0.30000000000000004\n",
            "epsisode: 302, total_steps: 9 episode reward: 0.20000000000000007\n",
            "epsisode: 303, total_steps: 15 episode reward: -0.40000000000000013\n",
            "epsisode: 304, total_steps: 19 episode reward: -0.8000000000000005\n",
            "epsisode: 305, total_steps: 15 episode reward: -0.40000000000000013\n",
            "epsisode: 306, total_steps: 16 episode reward: -0.5000000000000002\n",
            "epsisode: 307, total_steps: 11 episode reward: 1.1102230246251565e-16\n",
            "epsisode: 308, total_steps: 13 episode reward: -0.19999999999999996\n",
            "epsisode: 309, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 310, total_steps: 10 episode reward: 0.10000000000000009\n",
            "epsisode: 311, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 312, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 313, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 314, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 315, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 316, total_steps: 12 episode reward: -0.09999999999999987\n",
            "epsisode: 317, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 318, total_steps: 13 episode reward: -0.19999999999999996\n",
            "epsisode: 319, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 320, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 321, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 322, total_steps: 11 episode reward: 1.1102230246251565e-16\n",
            "epsisode: 323, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 324, total_steps: 15 episode reward: -0.40000000000000013\n",
            "epsisode: 325, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 326, total_steps: 15 episode reward: -0.40000000000000013\n",
            "epsisode: 327, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 328, total_steps: 17 episode reward: -0.6000000000000003\n",
            "epsisode: 329, total_steps: 19 episode reward: -0.8000000000000005\n",
            "epsisode: 330, total_steps: 18 episode reward: -0.7000000000000004\n",
            "epsisode: 331, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 332, total_steps: 15 episode reward: -0.40000000000000013\n",
            "epsisode: 333, total_steps: 18 episode reward: -0.7000000000000004\n",
            "epsisode: 334, total_steps: 12 episode reward: -0.09999999999999987\n",
            "epsisode: 335, total_steps: 18 episode reward: -0.7000000000000004\n",
            "epsisode: 336, total_steps: 15 episode reward: -0.40000000000000013\n",
            "epsisode: 337, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 338, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 339, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 340, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 341, total_steps: 12 episode reward: -0.09999999999999987\n",
            "epsisode: 342, total_steps: 17 episode reward: -0.6000000000000003\n",
            "epsisode: 343, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 344, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 345, total_steps: 13 episode reward: -0.19999999999999996\n",
            "epsisode: 346, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 347, total_steps: 14 episode reward: -0.30000000000000004\n",
            "epsisode: 348, total_steps: 12 episode reward: -0.09999999999999987\n",
            "epsisode: 349, total_steps: 17 episode reward: -0.6000000000000003\n",
            "epsisode: 350, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 351, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 352, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 353, total_steps: 10 episode reward: 0.10000000000000009\n",
            "epsisode: 354, total_steps: 15 episode reward: -0.40000000000000013\n",
            "epsisode: 355, total_steps: 13 episode reward: -0.19999999999999996\n",
            "epsisode: 356, total_steps: 10 episode reward: 0.10000000000000009\n",
            "epsisode: 357, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 358, total_steps: 16 episode reward: -0.5000000000000002\n",
            "epsisode: 359, total_steps: 11 episode reward: 1.1102230246251565e-16\n",
            "epsisode: 360, total_steps: 10 episode reward: 0.10000000000000009\n",
            "epsisode: 361, total_steps: 9 episode reward: 0.20000000000000007\n",
            "epsisode: 362, total_steps: 15 episode reward: -0.40000000000000013\n",
            "epsisode: 363, total_steps: 18 episode reward: -0.7000000000000004\n",
            "epsisode: 364, total_steps: 9 episode reward: 0.20000000000000007\n",
            "epsisode: 365, total_steps: 13 episode reward: -0.19999999999999996\n",
            "epsisode: 366, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 367, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 368, total_steps: 12 episode reward: -0.09999999999999987\n",
            "epsisode: 369, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 370, total_steps: 9 episode reward: 0.20000000000000007\n",
            "epsisode: 371, total_steps: 11 episode reward: 1.1102230246251565e-16\n",
            "epsisode: 372, total_steps: 13 episode reward: -0.19999999999999996\n",
            "epsisode: 373, total_steps: 13 episode reward: -0.19999999999999996\n",
            "epsisode: 374, total_steps: 17 episode reward: -0.6000000000000003\n",
            "epsisode: 375, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 376, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 377, total_steps: 16 episode reward: -0.5000000000000002\n",
            "epsisode: 378, total_steps: 11 episode reward: 1.1102230246251565e-16\n",
            "epsisode: 379, total_steps: 18 episode reward: -0.7000000000000004\n",
            "epsisode: 380, total_steps: 14 episode reward: -0.30000000000000004\n",
            "epsisode: 381, total_steps: 17 episode reward: -0.6000000000000003\n",
            "epsisode: 382, total_steps: 17 episode reward: -0.6000000000000003\n",
            "epsisode: 383, total_steps: 13 episode reward: -0.19999999999999996\n",
            "epsisode: 384, total_steps: 10 episode reward: 0.10000000000000009\n",
            "epsisode: 385, total_steps: 14 episode reward: -0.30000000000000004\n",
            "epsisode: 386, total_steps: 11 episode reward: 1.1102230246251565e-16\n",
            "epsisode: 387, total_steps: 14 episode reward: -0.30000000000000004\n",
            "epsisode: 388, total_steps: 17 episode reward: -0.6000000000000003\n",
            "epsisode: 389, total_steps: 13 episode reward: -0.19999999999999996\n",
            "epsisode: 390, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 391, total_steps: 14 episode reward: -0.30000000000000004\n",
            "epsisode: 392, total_steps: 13 episode reward: -0.19999999999999996\n",
            "epsisode: 393, total_steps: 11 episode reward: 1.1102230246251565e-16\n",
            "epsisode: 394, total_steps: 13 episode reward: -0.19999999999999996\n",
            "epsisode: 395, total_steps: 15 episode reward: -0.40000000000000013\n",
            "epsisode: 396, total_steps: 19 episode reward: -0.8000000000000005\n",
            "epsisode: 397, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 398, total_steps: 10 episode reward: 0.10000000000000009\n",
            "epsisode: 399, total_steps: 17 episode reward: -0.6000000000000003\n",
            "epsisode: 400, total_steps: 13 episode reward: -0.19999999999999996\n",
            "epsisode: 401, total_steps: 13 episode reward: -0.19999999999999996\n",
            "epsisode: 402, total_steps: 11 episode reward: 1.1102230246251565e-16\n",
            "epsisode: 403, total_steps: 11 episode reward: 1.1102230246251565e-16\n",
            "epsisode: 404, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 405, total_steps: 12 episode reward: -0.09999999999999987\n",
            "epsisode: 406, total_steps: 14 episode reward: -0.30000000000000004\n",
            "epsisode: 407, total_steps: 15 episode reward: -0.40000000000000013\n",
            "epsisode: 408, total_steps: 11 episode reward: 1.1102230246251565e-16\n",
            "epsisode: 409, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 410, total_steps: 14 episode reward: -0.30000000000000004\n",
            "epsisode: 411, total_steps: 17 episode reward: -0.6000000000000003\n",
            "epsisode: 412, total_steps: 13 episode reward: -0.19999999999999996\n",
            "epsisode: 413, total_steps: 9 episode reward: 0.20000000000000007\n",
            "epsisode: 414, total_steps: 16 episode reward: -0.5000000000000002\n",
            "epsisode: 415, total_steps: 19 episode reward: -0.8000000000000005\n",
            "epsisode: 416, total_steps: 19 episode reward: -0.8000000000000005\n",
            "epsisode: 417, total_steps: 13 episode reward: -0.19999999999999996\n",
            "epsisode: 418, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 419, total_steps: 14 episode reward: -0.30000000000000004\n",
            "epsisode: 420, total_steps: 18 episode reward: -0.7000000000000004\n",
            "epsisode: 421, total_steps: 12 episode reward: -0.09999999999999987\n",
            "epsisode: 422, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 423, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 424, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 425, total_steps: 16 episode reward: -0.5000000000000002\n",
            "epsisode: 426, total_steps: 13 episode reward: -0.19999999999999996\n",
            "epsisode: 427, total_steps: 14 episode reward: -0.30000000000000004\n",
            "epsisode: 428, total_steps: 15 episode reward: -0.40000000000000013\n",
            "epsisode: 429, total_steps: 13 episode reward: -0.19999999999999996\n",
            "epsisode: 430, total_steps: 10 episode reward: 0.10000000000000009\n",
            "epsisode: 431, total_steps: 15 episode reward: -0.40000000000000013\n",
            "epsisode: 432, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 433, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 434, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 435, total_steps: 14 episode reward: -0.30000000000000004\n",
            "epsisode: 436, total_steps: 15 episode reward: -0.40000000000000013\n",
            "epsisode: 437, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 438, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 439, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 440, total_steps: 19 episode reward: -0.8000000000000005\n",
            "epsisode: 441, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 442, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 443, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 444, total_steps: 11 episode reward: 1.1102230246251565e-16\n",
            "epsisode: 445, total_steps: 11 episode reward: 1.1102230246251565e-16\n",
            "epsisode: 446, total_steps: 15 episode reward: -0.40000000000000013\n",
            "epsisode: 447, total_steps: 11 episode reward: 1.1102230246251565e-16\n",
            "epsisode: 448, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 449, total_steps: 13 episode reward: -0.19999999999999996\n",
            "epsisode: 450, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 451, total_steps: 14 episode reward: -0.30000000000000004\n",
            "epsisode: 452, total_steps: 15 episode reward: -0.40000000000000013\n",
            "epsisode: 453, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 454, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 455, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 456, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 457, total_steps: 19 episode reward: -0.8000000000000005\n",
            "epsisode: 458, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 459, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 460, total_steps: 15 episode reward: -0.40000000000000013\n",
            "epsisode: 461, total_steps: 14 episode reward: -0.30000000000000004\n",
            "epsisode: 462, total_steps: 18 episode reward: -0.7000000000000004\n",
            "epsisode: 463, total_steps: 15 episode reward: -0.40000000000000013\n",
            "epsisode: 464, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 465, total_steps: 17 episode reward: -0.6000000000000003\n",
            "epsisode: 466, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 467, total_steps: 10 episode reward: 0.10000000000000009\n",
            "epsisode: 468, total_steps: 16 episode reward: -0.5000000000000002\n",
            "epsisode: 469, total_steps: 10 episode reward: 0.10000000000000009\n",
            "epsisode: 470, total_steps: 14 episode reward: -0.30000000000000004\n",
            "epsisode: 471, total_steps: 16 episode reward: -0.5000000000000002\n",
            "epsisode: 472, total_steps: 11 episode reward: 1.1102230246251565e-16\n",
            "epsisode: 473, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 474, total_steps: 16 episode reward: -0.5000000000000002\n",
            "epsisode: 475, total_steps: 14 episode reward: -0.30000000000000004\n",
            "epsisode: 476, total_steps: 17 episode reward: -0.6000000000000003\n",
            "epsisode: 477, total_steps: 12 episode reward: -0.09999999999999987\n",
            "epsisode: 478, total_steps: 13 episode reward: -0.19999999999999996\n",
            "epsisode: 479, total_steps: 15 episode reward: -0.40000000000000013\n",
            "epsisode: 480, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 481, total_steps: 11 episode reward: 1.1102230246251565e-16\n",
            "epsisode: 482, total_steps: 15 episode reward: -0.40000000000000013\n",
            "epsisode: 483, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 484, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 485, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 486, total_steps: 15 episode reward: -0.40000000000000013\n",
            "epsisode: 487, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 488, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 489, total_steps: 19 episode reward: -0.8000000000000005\n",
            "epsisode: 490, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 491, total_steps: 12 episode reward: -0.09999999999999987\n",
            "epsisode: 492, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 493, total_steps: 14 episode reward: -0.30000000000000004\n",
            "epsisode: 494, total_steps: 14 episode reward: -0.30000000000000004\n",
            "epsisode: 495, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 496, total_steps: 19 episode reward: -1.9000000000000006\n",
            "epsisode: 497, total_steps: 10 episode reward: 0.10000000000000009\n",
            "epsisode: 498, total_steps: 17 episode reward: -0.6000000000000003\n",
            "epsisode: 499, total_steps: 11 episode reward: 1.1102230246251565e-16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sc-52JbiHc-U",
        "outputId": "3a697d44-7dc0-4965-e6c9-30368cf2980f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "Qtable"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>right</th>\n",
              "      <th>left</th>\n",
              "      <th>up</th>\n",
              "      <th>down</th>\n",
              "      <th>forward</th>\n",
              "      <th>backward</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>000</th>\n",
              "      <td>-0.809712</td>\n",
              "      <td>-0.800342</td>\n",
              "      <td>-0.527926</td>\n",
              "      <td>-0.787894</td>\n",
              "      <td>-0.781186</td>\n",
              "      <td>-0.799240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>001</th>\n",
              "      <td>-0.682267</td>\n",
              "      <td>-0.657109</td>\n",
              "      <td>-0.368586</td>\n",
              "      <td>-0.712077</td>\n",
              "      <td>-0.610397</td>\n",
              "      <td>-0.576017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>002</th>\n",
              "      <td>-0.391241</td>\n",
              "      <td>-0.501061</td>\n",
              "      <td>-0.498746</td>\n",
              "      <td>-0.477856</td>\n",
              "      <td>-0.515323</td>\n",
              "      <td>-0.466786</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>003</th>\n",
              "      <td>-0.280891</td>\n",
              "      <td>-0.267380</td>\n",
              "      <td>-0.271771</td>\n",
              "      <td>-0.274111</td>\n",
              "      <td>-0.121716</td>\n",
              "      <td>-0.272016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>010</th>\n",
              "      <td>-0.722099</td>\n",
              "      <td>-0.752752</td>\n",
              "      <td>-0.526656</td>\n",
              "      <td>-0.738973</td>\n",
              "      <td>-0.714985</td>\n",
              "      <td>-0.735608</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>323</th>\n",
              "      <td>0.993642</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.216700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>330</th>\n",
              "      <td>-0.247512</td>\n",
              "      <td>-0.241927</td>\n",
              "      <td>-0.059868</td>\n",
              "      <td>-0.234897</td>\n",
              "      <td>-0.250952</td>\n",
              "      <td>-0.231425</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>331</th>\n",
              "      <td>-0.179576</td>\n",
              "      <td>-0.155377</td>\n",
              "      <td>0.130543</td>\n",
              "      <td>-0.149251</td>\n",
              "      <td>-0.173628</td>\n",
              "      <td>-0.151634</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>332</th>\n",
              "      <td>-0.050000</td>\n",
              "      <td>0.099173</td>\n",
              "      <td>-0.050000</td>\n",
              "      <td>-0.049800</td>\n",
              "      <td>0.265966</td>\n",
              "      <td>-0.057133</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>333</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>64 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        right      left        up      down   forward  backward\n",
              "000 -0.809712 -0.800342 -0.527926 -0.787894 -0.781186 -0.799240\n",
              "001 -0.682267 -0.657109 -0.368586 -0.712077 -0.610397 -0.576017\n",
              "002 -0.391241 -0.501061 -0.498746 -0.477856 -0.515323 -0.466786\n",
              "003 -0.280891 -0.267380 -0.271771 -0.274111 -0.121716 -0.272016\n",
              "010 -0.722099 -0.752752 -0.526656 -0.738973 -0.714985 -0.735608\n",
              "..        ...       ...       ...       ...       ...       ...\n",
              "323  0.993642  0.000000  0.000000  0.000000  0.000000  0.216700\n",
              "330 -0.247512 -0.241927 -0.059868 -0.234897 -0.250952 -0.231425\n",
              "331 -0.179576 -0.155377  0.130543 -0.149251 -0.173628 -0.151634\n",
              "332 -0.050000  0.099173 -0.050000 -0.049800  0.265966 -0.057133\n",
              "333  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000\n",
              "\n",
              "[64 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCznejKzKX0C",
        "outputId": "934269bc-4954-4cfd-95f3-c34c7c084272",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# # Evaluate agent's performance after Q-learning\n",
        "# # Qtable alrdy done\n",
        "\n",
        "# total_epochs, total_penalties = 0, 0\n",
        "# episodes = 100\n",
        "\n",
        "# for _ in range(episodes):\n",
        "#     state = env.reset()\n",
        "#     epochs, penalties, reward = 0, 0, 0\n",
        "    \n",
        "#     done = False\n",
        "    \n",
        "#     while not done:\n",
        "#         action = Qtable.loc[state].idxmax() # get index of highest action\n",
        "#         reward, done, next_state = env.step(action)\n",
        "\n",
        "#         if reward == -10:\n",
        "#             penalties += 1\n",
        "\n",
        "#         epochs += 1\n",
        "\n",
        "#     total_penalties += penalties\n",
        "#     total_epochs += epochs\n",
        "\n",
        "# print(f\"Results after {episodes} episodes:\")\n",
        "# print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
        "# print(f\"Average penalties per episode: {total_penalties / episodes}\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results after 100 episodes:\n",
            "Average timesteps per episode: 18.97\n",
            "Average penalties per episode: 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dgrSxM-L4Rh",
        "outputId": "f16c117e-6b8a-4f4b-c8e2-4ea6c55a1a96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "# show \"path\"\n",
        "# Qtable is populated\n",
        "\n",
        "env = TreasureCube()\n",
        "\n",
        "end_state = \"333\"\n",
        "is_end_state = False;\n",
        "i = 1\n",
        "\n",
        "state = env.reset()\n",
        "\n",
        "while not is_end_state:\n",
        "  print(\"=== Step\", i, \"\\tcurrent state =\" , state)\n",
        "#  env.render() # render\n",
        "\n",
        "  # check if terminal state\n",
        "  if(state == end_state):\n",
        "    print(\"*** Reach end state.\")\n",
        "    break\n",
        "\n",
        "  # choose best action of this state\n",
        "  action = Qtable.loc[state].idxmax() # get index of highest action\n",
        "\n",
        "  reward, terminate, next_state = env.step(action)\n",
        "  # next_state = env.getNextState(state, action) # TODO DOESNT work T_T\n",
        "\n",
        "  print(\"\\tTake action =\", action, \", next state =\",next_state)\n",
        "\n",
        "  # go to next state\n",
        "  state = next_state\n",
        "  i += 1\n",
        "\n",
        "  # # check if terminal state\n",
        "  # if(state == end_state):\n",
        "  #   print(\"*** Reach end state.\")\n",
        "  #   is_end_state = True\n",
        "\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=== Step 1 \tcurrent state = 000\n",
            "\tTake action = up , next state = 000\n",
            "=== Step 2 \tcurrent state = 000\n",
            "\tTake action = up , next state = 001\n",
            "=== Step 3 \tcurrent state = 001\n",
            "\tTake action = up , next state = 101\n",
            "=== Step 4 \tcurrent state = 101\n",
            "\tTake action = forward , next state = 201\n",
            "=== Step 5 \tcurrent state = 201\n",
            "\tTake action = forward , next state = 200\n",
            "=== Step 6 \tcurrent state = 200\n",
            "\tTake action = forward , next state = 300\n",
            "=== Step 7 \tcurrent state = 300\n",
            "\tTake action = up , next state = 301\n",
            "=== Step 8 \tcurrent state = 301\n",
            "\tTake action = right , next state = 301\n",
            "=== Step 9 \tcurrent state = 301\n",
            "\tTake action = right , next state = 311\n",
            "=== Step 10 \tcurrent state = 311\n",
            "\tTake action = right , next state = 321\n",
            "=== Step 11 \tcurrent state = 321\n",
            "\tTake action = right , next state = 331\n",
            "=== Step 12 \tcurrent state = 331\n",
            "\tTake action = up , next state = 332\n",
            "=== Step 13 \tcurrent state = 332\n",
            "\tTake action = forward , next state = 333\n",
            "=== Step 14 \tcurrent state = 333\n",
            "*** Reach end state.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7F7sMnA5RuzG",
        "outputId": "57698412-2f1c-4750-b276-2adc465619c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "Qtable.loc[\"000\"]"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "right      -0.809712\n",
              "left       -0.800342\n",
              "up         -0.527926\n",
              "down       -0.787894\n",
              "forward    -0.781186\n",
              "backward   -0.799240\n",
              "Name: 000, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__1ovgtPiA9a"
      },
      "source": [
        "# def test_cube(max_episode, max_step):\n",
        "#     env = TreasureCube(max_step=max_step)\n",
        "#     agent = RandomAgent() # TODO replace ??\n",
        "\n",
        "#     for epsisode_num in range(0, max_episode):\n",
        "#         state = env.reset()\n",
        "#         terminate = False\n",
        "#         t = 0\n",
        "#         episode_reward = 0\n",
        "#         while not terminate:\n",
        "#             action = agent.take_action(state)\n",
        "#             reward, terminate, next_state = env.step(action)\n",
        "#             episode_reward += reward\n",
        "#             # you can comment the following two lines, if the output is too much\n",
        "#             env.render() # comment\n",
        "#             print(f'step: {t}, action: {action}, reward: {reward}') # comment\n",
        "#             t += 1\n",
        "#             agent.train(state, action, next_state, reward)\n",
        "#             state = next_state\n",
        "#         print(f'epsisode: {epsisode_num}, total_steps: {t} episode reward: {episode_reward}')\n",
        "\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     parser = argparse.ArgumentParser(description='Test')\n",
        "#     parser.add_argument('--max_episode', type=int, default=500)\n",
        "#     parser.add_argument('--max_step', type=int, default=500)\n",
        "#     args = parser.parse_args()\n",
        "\n",
        "#     test_cube(args.max_episode, args.max_step)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}